***         INTRODUCTION            ***
One fundamental attribute of interepreters is that they take source coude and evaluate it without producing some visible, intermediate results that can later be executed.
Compilers take source code and produce output in another language that the underlying system can understand
Some compilers are tiny and dont even parse
some are more advanced and also compile it into an internal representation called bytecode and evaluate that
More advanced, JIT interepreters compile the input just-in-time into native machine code that gets executed

**This interepreter will parse the source code, build an abstract syntax tree(AST) and then evaluate the tree

WE WILL BUILD: lexer, parser, tree representation, and evaluator.

WE WILL LEARN: What "tokens" do, what an AST is and how to build it, how to extend our language with new data structures and functions

Monkey features - C-like syntax, variable bindings, ints and bools, arithmetic expressions, built-in functions, first-class and higher-order functions
closures, string data structure, array data structure, hash data structure.

steps: tokenize and parse Monkey source code in a REPL -> build an internal representation of the code called AST and then evaluate tree.

Creation List: Lexer -> Parser -> abstract syntax tree (AST) -> Internal object system -> evaluator

**WHY GO - easy to read and understand, universal formatting style thanks to gofmt, and no other tools besides GO needed

***         CHAPTER 1: LEXING           ***
    Lexical analysis:
Source code -> Tokens -> abstract syntax tree
Source to tokens is called lexical analysis or lexing
Called a lexer or tokenizer or scanner
Tokens - small, easy categorizable data structures that are then fed to the parser
White space will not show up as a token, but some languages like python they are significant

    Defining our tokens:
numbers like 5 or 10 will be integers, variable names will be identifiers, and words like variables but not identifiers will be keywords

Package token
type TokenType string
type Token struct {
    Type TokenType      //distinguish between types like "integer" or "right bracket"
    Literal String      //Holds value of the token
}

We use string because it allows us to use many different values, easy to debug, and able to print it. int or byte could be more efficient but not necessary
TWO SPECIAL TYPES:
ILLEGAL- token/character that we dont know about
EOF- end of file, tells our parser to stop

    The Lexer:
Does not need to buffer or save tokens, since there is only one method NextToken(), which outputs the next token
two pointers in Lexer struct, position and readPostion
position - points to the character in the input that corresponds to the ch byte
readPostion points to the next character in the input
readChar() - gives us the next character and advances our position in the input
    First, checks if we have reached the end. if so, set l.ch to 0 which is the ASCII code for "NUL", either means we havent read anything or EOF
    l.position becomes l.readPostion and l.readPostion is incremented by one
    only supports ASCII characters instead of full unicode to keep things simple. To fully support unicode and UTF-8 change l.ch from byte to rune.
NextToken() - look at current character (l.ch) and return a token depending on the character.
use readChar() to advance our pointer before returning the current token
newToken() - helps initialize new tokens

Identifiers and keywords - lexer needs to recognize whether the current character is a letter and if so, read the rest untill it encounters a non - letter to identify if it is an identifier or keywords
default branch to check for indentifiers when l.ch is not recognized
ILLEGAL tokens for when we dont know how to handle the current character
isLetter() - checks whether the given argument is a letter. Allows for things like _ to be included in indentifiers/keywords, such as foo_bar
readIdentifier() - reads in an identifier and advances our lexer's position untill it encounters a non-letter-character
token literal will be set by readIdentifier() in the default branch
Create a map of identifiers that will be used as keywords
LookupIdent() - check if identifier is in keyword table, if so return keywords TokenType constant, if not return token.IDENT which is the TokenType for all user-defined identifiers
skipWhitespace() - if there are any characters to skip, such as as ' ' or '\t', call l.readChar() to move pointer to next character
Some languages create tokens for newline characters, and some throw parsing errors if they are in the incorrect spot.
readNumber() - reads in a number and advances our lexers position // exactly the same as readIdentifier but it uses isDigit() instead of isLetter()
isDigit() - returns whether ch is a number between 0 and 9

**The lexer's job is not to tell us if the code makes sense, works or contains errors, it should only turn the input into tokens

Adding [-,!,*,/,<,>] Tokens: Add them to token constants list, and then NextToken() switch case
Adding [true,false,if,else,return] Tokens: add them to token constants list and the keywords map
Adding [==,!=] tokens: reuse '=' and '!' branches and extend them by looking at the next character, and also add to consstants list

peekChar() - returns the next char but does not increment l.position    
When checking for two-character tokens, make sure to save the current char before calling readChar() 

** The difficulty of parsing different languages often comes down to how far you have to look forwards or backwards

START OF A REPL (Read Evaluate Print Loop)
REPL will read in the tokens and print them out untill the EOF token is produced

***       CHAPTER 2: PARSING        ***
A parser turns its input into a data structure that represents the input
//Side Note: in Lisp, the data structures used to represent the source code are the ones used by Lisp users. "Code is data, data is code" - commonly said by Lisp programmers
The process of parsing is also called snytactic analysis
** Take source code, build data structure, analyze input and check that it conforms to the expected structure while building data structure

** Parser Notes **
Common Parser Generators: yacc, bison, ANTLR
 - tools that when fed a formal description of a language, produce parsers
 Context-free grammar (CFG) - a set of rules that describe how to form correct sentences in a language
    - Most common CFS's are Backus-Naur Form (BNF) or Extended Backus-Naur Form (EBNF)
Top down parsing: "recursive descent parsing", "early parsing", or "predictive parsing"

We will be using a recusive descent parser, "top down operator presedence" parser, or "Pratt parser" after Vaughan Pratt
This means we will be starting with the constructing root node, and working our way down

    Parsing Let Statements:
let x = 10;    can be boiled down to let <identifier> = <expression>;
Expressions produce values.     //the expression 5 has a value of 5
Statements do not.              //let x = 5; or return 5; do not produce a value

Three interfaces, Node, Statement, and Expression
Every node in our AST will implement the Node interface, which provides a TokenLiteral() method that returns the literal value of the token it is associated with
TokenLiteral() - used for debugging
statementNode and expressionNode are for guiding the Go compiler and throwing errors if a statement is used when an expression should be used and vice versa
Program node will be the root node for every AST our parser produces

**What does a node for a variable binding need?
    - It needs one for the name of the variable
    - a field that points to the expression on the right side of the equal sign // it needs to point to any expression
    - it needs to keep track of the toke the AST node is associated with
    - Identifier, expression that produces value in let statement, and token

Parser: l, curToken, and peekToken
    l - pointer to an instance of the lexer
    curToken - pointer to our current token         //like position in our lexer
    peekToken - pointer to the next token           //like readPosition in our lexer

We need to look at the curToken, decide what to do next, and use peekToken if curToken does not give us enough information
Parser Pseudo - construct root node, build child nodes, the statements, by calling other functions that know which node to construct based on current token
recursion happens inside parser functions since we dont know what might need to be parsed next
Ex: for 5+5 we first parse 5 + and then we call parseExpression() again since it could be 5+ 5 *10
Parser - repeatedly advance the oken, and check the current token to decide what to do next: either call another function or thrown an error
ParseProgram() - construct root node -> iterate over tokens untill EOF -> add all statements to Statements slice of root node
parseLetStatement() - construct *ast.LetStatement node with current token, advances token an expects IDENT token, which it constructs and Identifier node
    then it expects an equal sign and jumps over the expression untill it encounters a semicolon, will be replaced by parsing expressions
curTokenis(x) - check if current token is x
peekTokenIs(x) - check if next token is x
expectPeek() - checks type of peekToken() and if it is correct, advance the token by calling NextToken()
    - expectPeek() is an "assertion function" most parsers share
    it enforces the correctness of the order of tokens by checking the next ones type
Adding Errors checking for debugging:
    add and error field in the parser, which is a slice of strings
    peekError() - print out expected token vs recieved token, and add error to errors
    Errors() - check if parser encountered any errors
    add peekError() to expectPeek() to add an error every time our token expectation is wrong

***PARSING RETURN STATEMENENTS***
First step is to define the necessary structures in the ast Package
return statements consist solely of the keywords return and an expression
add a case for token.RETURN in our parseStatement() switch case
parseReturnStatement() - construct an ast.ReturnStatement, with the current token. Skips over ever expression untill a semicolon

***PARSING EXPRESSIONS***
Parsing statements are straight forward as we work left to right
expressions are more difficult as we have operator precedence issues such as 
    5+ 5*10 or (5+10) * 5
Cases like this require certain parts of an expression to be deeper into an AST, and evaluated earlier
Another issues is that the same tokens can be present in multiple positions such as:
    -5-10 or 5*(add(2,3)+10)
One set of parenthesis groups an expression while the other is a call expression\
*The validity of a token's position now depends on the context, tokens before/after, and their precedence
*Everything except let and return statements are an expression
EXPRESSIONS - prefix operators, infix operatiors, comparison operators, parentheses, call expressions, identifiers
Typically parsers use grammar rules (BNF or EBNF) but we will be using single token types
*Each token type can have two parsing functions associated with it, infix or prefix
String() - create a buffer, write return value of each statement's String() method, and then returns the buffer as a string