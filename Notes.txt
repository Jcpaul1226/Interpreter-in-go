***         INTRODUCTION            ***
One fundamental attribute of interepreters is that they take source coude and evaluate it without producing some visible, intermediate results that can later be executed.
Compilers take source code and produce output in another language that the underlying system can understand
Some compilers are tiny and dont even parse
some are more advanced and also compile it into an internal representation called bytecode and evaluate that
More advanced, JIT interepreters compile the input just-in-time into native machine code that gets executed

**This interepreter will parse the source code, build an abstract syntax tree(AST) and then evaluate the tree

WE WILL BUILD: lexer, parser, tree representation, and evaluator.

WE WILL LEARN: What "tokens" do, what an AST is and how to build it, how to extend our language with new data structures and functions

Monkey features - C-like syntax, variable bindings, ints and bools, arithmetic expressions, built-in functions, first-class and higher-order functions
closures, string data structure, array data structure, hash data structure.

steps: tokenize and parse Monkey source code in a REPL -> build an internal representation of the code called AST and then evaluate tree.

Creation List: Lexer -> Parser -> abstract syntax tree (AST) -> Internal object system -> evaluator

**WHY GO - easy to read and understand, universal formatting style thanks to gofmt, and no other tools besides GO needed

***         CHAPTER 1: LEXING           ***
    Lexical analysis:
Source code -> Tokens -> abstract syntax tree
Source to tokens is called lexical analysis or lexing
Called a lexer or tokenizer or scanner
Tokens - small, easy categorizable data structures that are then fed to the parser
White space will not show up as a token, but some languages like python they are significant

    Defining our tokens:
numbers like 5 or 10 will be integers, variable names will be identifiers, and words like variables but not identifiers will be keywords

Package token
type TokenType string
type Token struct {
    Type TokenType      //distinguish between types like "integer" or "right bracket"
    Literal String      //Holds value of the token
}

We use string because it allows us to use many different values, easy to debug, and able to print it. int or byte could be more efficient but not necessary
TWO SPECIAL TYPES:
ILLEGAL- token/character that we dont know about
EOF- end of file, tells our parser to stop

    The Lexer:
Does not need to buffer or save tokens, since there is only one method NextToken(), which outputs the next token
two pointers in Lexer struct, position and readPostion
position - points to the character in the input that corresponds to the ch byte
readPostion points to the next character in the input
readChar() - gives us the next character and advances our position in the input
    First, checks if we have reached the end. if so, set l.ch to 0 which is the ASCII code for "NUL", either means we havent read anything or EOF
    l.position becomes l.readPostion and l.readPostion is incremented by one
    only supports ASCII characters instead of full unicode to keep things simple. To fully support unicode and UTF-8 change l.ch from byte to rune.
NextToken() - look at current character (l.ch) and return a token depending on the character.
use readChar() to advance our pointer before returning the current token
newToken() - helps initialize new tokens

Identifiers and keywords - lexer needs to recognize whether the current character is a letter and if so, read the rest untill it encounters a non - letter to identify if it is an identifier or keywords
default branch to check for indentifiers when l.ch is not recognized
ILLEGAL tokens for when we dont know how to handle the current character
isLetter() - checks whether the given argument is a letter. Allows for things like _ to be included in indentifiers/keywords, such as foo_bar
readIdentifier() - reads in an identifier and advances our lexer's position untill it encounters a non-letter-character
token literal will be set by readIdentifier() in the default branch
Create a map of identifiers that will be used as keywords
LookupIdent() - check if identifier is in keyword table, if so return keywords TokenType constant, if not return token.IDENT which is the TokenType for all user-defined identifiers
skipWhitespace() - if there are any characters to skip, such as as ' ' or '\t', call l.readChar() to move pointer to next character
Some languages create tokens for newline characters, and some throw parsing errors if they are in the incorrect spot.
readNumber() - reads in a number and advances our lexers position // exactly the same as readIdentifier but it uses isDigit() instead of isLetter()
isDigit() - returns whether ch is a number between 0 and 9

**The lexer's job is not to tell us if the code makes sense, works or contains errors, it should only turn the input into tokens

Adding [-,!,*,/,<,>] Tokens: Add them to token constants list, and then NextToken() switch case
Adding [true,false,if,else,return] Tokens: add them to token constants list and the keywords map
Adding [==,!=] tokens: reuse '=' and '!' branches and extend them by looking at the next character, and also add to consstants list

peekChar() - returns the next char but does not increment l.position    
When checking for two-character tokens, make sure to save the current char before calling readChar() 

** The difficulty of parsing different languages often comes down to how far you have to look forwards or backwards

START OF A REPL (Read Evaluate Print Loop)
REPL will read in the tokens and print them out untill the EOF token is produced

***       CHAPTER 2: PARSING        ***
A parser turns its input into a data structure that represents the input
//Side Note: in Lisp, the data structures used to represent the source code are the ones used by Lisp users. "Code is data, data is code" - commonly said by Lisp programmers
The process of parsing is also called snytactic analysis
** Take source code, build data structure, analyze input and check that it conforms to the expected structure while building data structure

** Parser Notes **
Common Parser Generators: yacc, bison, ANTLR
 - tools that when fed a formal description of a language, produce parsers
 Context-free grammar (CFG) - a set of rules that describe how to form correct sentences in a language
    - Most common CFS's are Backus-Naur Form (BNF) or Extended Backus-Naur Form (EBNF)
Top down parsing: "recursive descent parsing", "early parsing", or "predictive parsing"

We will be using a recusive descent parser, "top down operator presedence" parser, or "Pratt parser" after Vaughan Pratt
This means we will be starting with the constructing root node, and working our way down

    Parsing Let Statements:
let x = 10;    can be boiled down to let <identifier> = <expression>;
Expressions produce values.     //the expression 5 has a value of 5
Statements do not.              //let x = 5; or return 5; do not produce a value
